{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance de chiffre en language des signes avec pytorch\n",
    "\n",
    "Dans cette exemple d'utilisation de deep learning, nous allons utiliser une base de donnée de 2061 images de mains faisant un chiffre en language des signes. Ces images sont en nuances de gris et dans une de plage de 0 à 1.\n",
    "Nous disposons également des vérités terrains associés à chaque image sous forme d'un tableau de taille 10 avec un 1 à l'index de la classe correspondante.\n",
    "\n",
    "Pour effectuer l'apprentissage nous allons utiliser pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  8828\n"
     ]
    }
   ],
   "source": [
    "#Import des modules pythons nécessaires\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "#Affichage de la graine d'aléatoire actuelle (afin de reproduire des résultats si nécessaire)\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "#Chargement de cuda\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'aller plus en profondeur avec des réseaux de neurone il faut tout d'abord charger les données.\n",
    "Pour cela nous chargons directement les images via numpy et pour et les vérités terrains nous avons décider de ramener le tableau à 1 unique valeur représentant l'axe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes :  0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Chargement des images\n",
    "Trans = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ])\n",
    "images = [Trans(Image.fromarray(im)) for im in np.load(\"./datas/X.npy\")]\n",
    "#Chargement des classes\n",
    "temp = np.load(\"./datas/Y.npy\")\n",
    "classes = np.zeros(len(images))\n",
    "for i in range(len(classes)):\n",
    "    classes[i] = temp[i].nonzero()[0][0]\n",
    "plt.imshow(transforms.ToPILImage()(images[0]))\n",
    "print(\"Classes : \",classes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir des datasets cohérents il faut aussi mélanger les données de bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mélange du dataset\n",
    "temp_s = []\n",
    "for i in range(len(classes)):\n",
    "    temp_s.append([images[i],classes[i] ])\n",
    "shuffle(temp_s)\n",
    "for i in range(len(classes)):\n",
    "    [im,c] = temp_s[i]\n",
    "    images[i] = im\n",
    "    classes[i] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons nore dataset il faut le partitionner pour avoir un ensemble pour l'entraînement\n",
    "et un pour les tests (afin de vérifier que notre réseau n'apprend pas par coeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition des partitions du dataset\n",
    "#Nous choisissons de prendre un dataset d'entrainement représentant 80% des données\n",
    "prc_train = int(0.80*len(images))\n",
    "prc_test = prc_train + int(0.2*len(images))\n",
    "#Chargement des partitions\n",
    "def load():\n",
    "    out = []\n",
    "    prc = [prc_train,prc_test]\n",
    "    for i in range(len(prc)):\n",
    "        if i == 0:\n",
    "            images_train = [(im) for im in images[:prc[i]]]\n",
    "            classes_train = classes[:prc_train]\n",
    "        #elif i == len(prc)-1:\n",
    "        #    images_train = [(im) for im in images[prc[i]:]]\n",
    "        #    classes_train = classes[prc[i]:]\n",
    "        else:\n",
    "            images_train = [(im) for im  in images[prc[i-1]:prc[i]]]\n",
    "            classes_train = classes[prc_train:prc_test]\n",
    "        tensor_x_train = torch.stack([torch.Tensor(i) for i in images_train])\n",
    "        tensor_y_train = torch.from_numpy(classes_train).long()\n",
    "        datas_train = utils.TensorDataset(tensor_x_train,tensor_y_train)\n",
    "        dataloader_train = utils.DataLoader(datas_train,50, shuffle=True)\n",
    "        out.append(dataloader_train)\n",
    "    return out\n",
    "[dataloader_train,dataloader_test] = load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite il faut définir les fonctions d'entraînements et de test associé pour entraîner nos réseaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataloader_train, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    for i, (data,target) in enumerate(dataloader_train, 0):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #print(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        #print(output)\n",
    "        loss.backward()\n",
    "        sum_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data), len(dataloader_train.dataset),\n",
    "                100. * i / len(dataloader_train), loss.item()))\n",
    "        return sum_loss / len(dataloader_train)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data,target) in enumerate(test_loader, 0):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous pouvons passer à la définition de notre réseau, nous allons tout d'abord commencer avec un perceptron multicouche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_FC, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32,200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,32*32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "criterion = nn.NLLLoss()\n",
    "model = Net_FC().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant temps de lancer notre premier apprentissage et voir les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1649 (0%)]\tLoss: 2.310808\n",
      "\n",
      "Test set: Average loss: 2.4525, Accuracy: 50/412 (12%)\n",
      "\n",
      "Train Epoch: 1 [0/1649 (0%)]\tLoss: 2.477517\n",
      "\n",
      "Test set: Average loss: 2.2792, Accuracy: 50/412 (12%)\n",
      "\n",
      "Train Epoch: 2 [0/1649 (0%)]\tLoss: 2.355103\n",
      "\n",
      "Test set: Average loss: 2.3081, Accuracy: 47/412 (11%)\n",
      "\n",
      "Train Epoch: 3 [0/1649 (0%)]\tLoss: 2.345956\n",
      "\n",
      "Test set: Average loss: 2.3116, Accuracy: 31/412 (8%)\n",
      "\n",
      "Train Epoch: 4 [0/1649 (0%)]\tLoss: 2.379469\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 5 [0/1649 (0%)]\tLoss: 2.310954\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 6 [0/1649 (0%)]\tLoss: 2.316044\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 7 [0/1649 (0%)]\tLoss: 2.319083\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 8 [0/1649 (0%)]\tLoss: 2.309026\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 9 [0/1649 (0%)]\tLoss: 2.306805\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 10 [0/1649 (0%)]\tLoss: 2.311194\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 11 [0/1649 (0%)]\tLoss: 2.307949\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 12 [0/1649 (0%)]\tLoss: 2.309382\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 13 [0/1649 (0%)]\tLoss: 2.302680\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 14 [0/1649 (0%)]\tLoss: 2.302657\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 15 [0/1649 (0%)]\tLoss: 2.290454\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 16 [0/1649 (0%)]\tLoss: 2.298577\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 17 [0/1649 (0%)]\tLoss: 2.303021\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 18 [0/1649 (0%)]\tLoss: 2.301640\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 19 [0/1649 (0%)]\tLoss: 2.303077\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 54/412 (13%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_arr = range(500)\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "#Entrainement\n",
    "for epoch in epoch_arr:\n",
    "    train_loss.append( train(model, dataloader_train, optimizer, epoch, F.nll_loss))\n",
    "    test_loss.append( test(model, dataloader_test) )\n",
    "#Affichage des courbes de loss\n",
    "plt.plot(epoch_arr,train_loss,'b')\n",
    "plt.plot(epoch_arr,test_loss,'g')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont très peu concluant, on peut maintenant esasyer avec un réseau à convolution que l'on termine avec 2 couches full connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, 1)\n",
    "        self.fc1 = nn.Linear(5*5*128, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5*5*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #Utilisation de dropout pour ajouter de l'aléatoire à l'entraienement\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "criterion = nn.NLLLoss()\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1649 (0%)]\tLoss: 2.293320\n",
      "\n",
      "Test set: Average loss: 5.7249, Accuracy: 32/412 (8%)\n",
      "\n",
      "Train Epoch: 1 [0/1649 (0%)]\tLoss: 6.472467\n",
      "\n",
      "Test set: Average loss: 2.5084, Accuracy: 54/412 (13%)\n",
      "\n",
      "Train Epoch: 2 [0/1649 (0%)]\tLoss: 2.609597\n",
      "\n",
      "Test set: Average loss: 2.3002, Accuracy: 39/412 (9%)\n",
      "\n",
      "Train Epoch: 3 [0/1649 (0%)]\tLoss: 2.305899\n",
      "\n",
      "Test set: Average loss: 2.2958, Accuracy: 39/412 (9%)\n",
      "\n",
      "Train Epoch: 4 [0/1649 (0%)]\tLoss: 2.319222\n",
      "\n",
      "Test set: Average loss: 2.2901, Accuracy: 59/412 (14%)\n",
      "\n",
      "Train Epoch: 5 [0/1649 (0%)]\tLoss: 2.279988\n",
      "\n",
      "Test set: Average loss: 2.2835, Accuracy: 69/412 (17%)\n",
      "\n",
      "Train Epoch: 6 [0/1649 (0%)]\tLoss: 2.267962\n",
      "\n",
      "Test set: Average loss: 2.2702, Accuracy: 64/412 (16%)\n",
      "\n",
      "Train Epoch: 7 [0/1649 (0%)]\tLoss: 2.265856\n",
      "\n",
      "Test set: Average loss: 2.2610, Accuracy: 48/412 (12%)\n",
      "\n",
      "Train Epoch: 8 [0/1649 (0%)]\tLoss: 2.276598\n",
      "\n",
      "Test set: Average loss: 2.2430, Accuracy: 59/412 (14%)\n",
      "\n",
      "Train Epoch: 9 [0/1649 (0%)]\tLoss: 2.305139\n",
      "\n",
      "Test set: Average loss: 2.2216, Accuracy: 123/412 (30%)\n",
      "\n",
      "Train Epoch: 10 [0/1649 (0%)]\tLoss: 2.188831\n",
      "\n",
      "Test set: Average loss: 2.2065, Accuracy: 100/412 (24%)\n",
      "\n",
      "Train Epoch: 11 [0/1649 (0%)]\tLoss: 2.226803\n",
      "\n",
      "Test set: Average loss: 2.1838, Accuracy: 84/412 (20%)\n",
      "\n",
      "Train Epoch: 12 [0/1649 (0%)]\tLoss: 2.267161\n",
      "\n",
      "Test set: Average loss: 2.1463, Accuracy: 81/412 (20%)\n",
      "\n",
      "Train Epoch: 13 [0/1649 (0%)]\tLoss: 2.181677\n",
      "\n",
      "Test set: Average loss: 2.1112, Accuracy: 106/412 (26%)\n",
      "\n",
      "Train Epoch: 14 [0/1649 (0%)]\tLoss: 2.108151\n",
      "\n",
      "Test set: Average loss: 2.0795, Accuracy: 144/412 (35%)\n",
      "\n",
      "Train Epoch: 15 [0/1649 (0%)]\tLoss: 2.053720\n",
      "\n",
      "Test set: Average loss: 2.0518, Accuracy: 126/412 (31%)\n",
      "\n",
      "Train Epoch: 16 [0/1649 (0%)]\tLoss: 2.061489\n",
      "\n",
      "Test set: Average loss: 1.9830, Accuracy: 139/412 (34%)\n",
      "\n",
      "Train Epoch: 17 [0/1649 (0%)]\tLoss: 2.010009\n",
      "\n",
      "Test set: Average loss: 1.8952, Accuracy: 146/412 (35%)\n",
      "\n",
      "Train Epoch: 18 [0/1649 (0%)]\tLoss: 1.999512\n",
      "\n",
      "Test set: Average loss: 1.7956, Accuracy: 195/412 (47%)\n",
      "\n",
      "Train Epoch: 19 [0/1649 (0%)]\tLoss: 1.879483\n",
      "\n",
      "Test set: Average loss: 1.7342, Accuracy: 171/412 (42%)\n",
      "\n",
      "Train Epoch: 20 [0/1649 (0%)]\tLoss: 1.675715\n",
      "\n",
      "Test set: Average loss: 1.6970, Accuracy: 143/412 (35%)\n",
      "\n",
      "Train Epoch: 21 [0/1649 (0%)]\tLoss: 1.794005\n",
      "\n",
      "Test set: Average loss: 1.5735, Accuracy: 197/412 (48%)\n",
      "\n",
      "Train Epoch: 22 [0/1649 (0%)]\tLoss: 1.839209\n",
      "\n",
      "Test set: Average loss: 1.4798, Accuracy: 245/412 (59%)\n",
      "\n",
      "Train Epoch: 23 [0/1649 (0%)]\tLoss: 1.559618\n",
      "\n",
      "Test set: Average loss: 1.4263, Accuracy: 226/412 (55%)\n",
      "\n",
      "Train Epoch: 24 [0/1649 (0%)]\tLoss: 1.454691\n",
      "\n",
      "Test set: Average loss: 1.3657, Accuracy: 240/412 (58%)\n",
      "\n",
      "Train Epoch: 25 [0/1649 (0%)]\tLoss: 1.421630\n",
      "\n",
      "Test set: Average loss: 1.3048, Accuracy: 254/412 (62%)\n",
      "\n",
      "Train Epoch: 26 [0/1649 (0%)]\tLoss: 1.283762\n",
      "\n",
      "Test set: Average loss: 1.3357, Accuracy: 209/412 (51%)\n",
      "\n",
      "Train Epoch: 27 [0/1649 (0%)]\tLoss: 1.296478\n",
      "\n",
      "Test set: Average loss: 1.2591, Accuracy: 226/412 (55%)\n",
      "\n",
      "Train Epoch: 28 [0/1649 (0%)]\tLoss: 1.282254\n",
      "\n",
      "Test set: Average loss: 1.1017, Accuracy: 286/412 (69%)\n",
      "\n",
      "Train Epoch: 29 [0/1649 (0%)]\tLoss: 1.332230\n",
      "\n",
      "Test set: Average loss: 1.1165, Accuracy: 264/412 (64%)\n",
      "\n",
      "Train Epoch: 30 [0/1649 (0%)]\tLoss: 1.331251\n",
      "\n",
      "Test set: Average loss: 1.1186, Accuracy: 259/412 (63%)\n",
      "\n",
      "Train Epoch: 31 [0/1649 (0%)]\tLoss: 1.173396\n",
      "\n",
      "Test set: Average loss: 1.0776, Accuracy: 266/412 (65%)\n",
      "\n",
      "Train Epoch: 32 [0/1649 (0%)]\tLoss: 1.202156\n",
      "\n",
      "Test set: Average loss: 1.0744, Accuracy: 267/412 (65%)\n",
      "\n",
      "Train Epoch: 33 [0/1649 (0%)]\tLoss: 1.025749\n",
      "\n",
      "Test set: Average loss: 1.1060, Accuracy: 264/412 (64%)\n",
      "\n",
      "Train Epoch: 34 [0/1649 (0%)]\tLoss: 1.111002\n",
      "\n",
      "Test set: Average loss: 1.0791, Accuracy: 261/412 (63%)\n",
      "\n",
      "Train Epoch: 35 [0/1649 (0%)]\tLoss: 0.959035\n",
      "\n",
      "Test set: Average loss: 0.9658, Accuracy: 276/412 (67%)\n",
      "\n",
      "Train Epoch: 36 [0/1649 (0%)]\tLoss: 0.905432\n",
      "\n",
      "Test set: Average loss: 0.8938, Accuracy: 302/412 (73%)\n",
      "\n",
      "Train Epoch: 37 [0/1649 (0%)]\tLoss: 1.199880\n",
      "\n",
      "Test set: Average loss: 0.8746, Accuracy: 311/412 (75%)\n",
      "\n",
      "Train Epoch: 38 [0/1649 (0%)]\tLoss: 0.789214\n",
      "\n",
      "Test set: Average loss: 0.8810, Accuracy: 314/412 (76%)\n",
      "\n",
      "Train Epoch: 39 [0/1649 (0%)]\tLoss: 1.075601\n",
      "\n",
      "Test set: Average loss: 0.8709, Accuracy: 324/412 (79%)\n",
      "\n",
      "Train Epoch: 40 [0/1649 (0%)]\tLoss: 1.185547\n",
      "\n",
      "Test set: Average loss: 0.8422, Accuracy: 317/412 (77%)\n",
      "\n",
      "Train Epoch: 41 [0/1649 (0%)]\tLoss: 0.696173\n",
      "\n",
      "Test set: Average loss: 0.8335, Accuracy: 308/412 (75%)\n",
      "\n",
      "Train Epoch: 42 [0/1649 (0%)]\tLoss: 0.684688\n",
      "\n",
      "Test set: Average loss: 0.8501, Accuracy: 302/412 (73%)\n",
      "\n",
      "Train Epoch: 43 [0/1649 (0%)]\tLoss: 0.848000\n",
      "\n",
      "Test set: Average loss: 0.9021, Accuracy: 289/412 (70%)\n",
      "\n",
      "Train Epoch: 44 [0/1649 (0%)]\tLoss: 0.740561\n",
      "\n",
      "Test set: Average loss: 0.9290, Accuracy: 280/412 (68%)\n",
      "\n",
      "Train Epoch: 45 [0/1649 (0%)]\tLoss: 0.861666\n",
      "\n",
      "Test set: Average loss: 0.8291, Accuracy: 301/412 (73%)\n",
      "\n",
      "Train Epoch: 46 [0/1649 (0%)]\tLoss: 0.932971\n",
      "\n",
      "Test set: Average loss: 0.7301, Accuracy: 315/412 (76%)\n",
      "\n",
      "Train Epoch: 47 [0/1649 (0%)]\tLoss: 1.063521\n",
      "\n",
      "Test set: Average loss: 0.6610, Accuracy: 332/412 (81%)\n",
      "\n",
      "Train Epoch: 48 [0/1649 (0%)]\tLoss: 0.747277\n",
      "\n",
      "Test set: Average loss: 0.7032, Accuracy: 331/412 (80%)\n",
      "\n",
      "Train Epoch: 49 [0/1649 (0%)]\tLoss: 0.764139\n",
      "\n",
      "Test set: Average loss: 0.7273, Accuracy: 323/412 (78%)\n",
      "\n",
      "Train Epoch: 50 [0/1649 (0%)]\tLoss: 0.639570\n",
      "\n",
      "Test set: Average loss: 0.6790, Accuracy: 329/412 (80%)\n",
      "\n",
      "Train Epoch: 51 [0/1649 (0%)]\tLoss: 0.851748\n",
      "\n",
      "Test set: Average loss: 0.6277, Accuracy: 341/412 (83%)\n",
      "\n",
      "Train Epoch: 52 [0/1649 (0%)]\tLoss: 0.818974\n",
      "\n",
      "Test set: Average loss: 0.6295, Accuracy: 335/412 (81%)\n",
      "\n",
      "Train Epoch: 53 [0/1649 (0%)]\tLoss: 0.809165\n",
      "\n",
      "Test set: Average loss: 0.6305, Accuracy: 327/412 (79%)\n",
      "\n",
      "Train Epoch: 54 [0/1649 (0%)]\tLoss: 1.171188\n",
      "\n",
      "Test set: Average loss: 0.6032, Accuracy: 336/412 (82%)\n",
      "\n",
      "Train Epoch: 55 [0/1649 (0%)]\tLoss: 0.740297\n",
      "\n",
      "Test set: Average loss: 0.6134, Accuracy: 338/412 (82%)\n",
      "\n",
      "Train Epoch: 56 [0/1649 (0%)]\tLoss: 0.775806\n",
      "\n",
      "Test set: Average loss: 0.6323, Accuracy: 327/412 (79%)\n",
      "\n",
      "Train Epoch: 57 [0/1649 (0%)]\tLoss: 0.456889\n",
      "\n",
      "Test set: Average loss: 0.6415, Accuracy: 330/412 (80%)\n",
      "\n",
      "Train Epoch: 58 [0/1649 (0%)]\tLoss: 0.670647\n",
      "\n",
      "Test set: Average loss: 0.6553, Accuracy: 333/412 (81%)\n",
      "\n",
      "Train Epoch: 59 [0/1649 (0%)]\tLoss: 0.612820\n",
      "\n",
      "Test set: Average loss: 0.6476, Accuracy: 333/412 (81%)\n",
      "\n",
      "Train Epoch: 60 [0/1649 (0%)]\tLoss: 0.605067\n",
      "\n",
      "Test set: Average loss: 0.6512, Accuracy: 333/412 (81%)\n",
      "\n",
      "Train Epoch: 61 [0/1649 (0%)]\tLoss: 0.666225\n",
      "\n",
      "Test set: Average loss: 0.6362, Accuracy: 335/412 (81%)\n",
      "\n",
      "Train Epoch: 62 [0/1649 (0%)]\tLoss: 0.672925\n",
      "\n",
      "Test set: Average loss: 0.6100, Accuracy: 341/412 (83%)\n",
      "\n",
      "Train Epoch: 63 [0/1649 (0%)]\tLoss: 0.549236\n",
      "\n",
      "Test set: Average loss: 0.5863, Accuracy: 346/412 (84%)\n",
      "\n",
      "Train Epoch: 64 [0/1649 (0%)]\tLoss: 0.743403\n",
      "\n",
      "Test set: Average loss: 0.5674, Accuracy: 346/412 (84%)\n",
      "\n",
      "Train Epoch: 65 [0/1649 (0%)]\tLoss: 0.660507\n",
      "\n",
      "Test set: Average loss: 0.5584, Accuracy: 342/412 (83%)\n",
      "\n",
      "Train Epoch: 66 [0/1649 (0%)]\tLoss: 0.426254\n",
      "\n",
      "Test set: Average loss: 0.5592, Accuracy: 335/412 (81%)\n",
      "\n",
      "Train Epoch: 67 [0/1649 (0%)]\tLoss: 0.535321\n",
      "\n",
      "Test set: Average loss: 0.5639, Accuracy: 332/412 (81%)\n",
      "\n",
      "Train Epoch: 68 [0/1649 (0%)]\tLoss: 0.604185\n",
      "\n",
      "Test set: Average loss: 0.5531, Accuracy: 333/412 (81%)\n",
      "\n",
      "Train Epoch: 69 [0/1649 (0%)]\tLoss: 0.411881\n",
      "\n",
      "Test set: Average loss: 0.5429, Accuracy: 338/412 (82%)\n",
      "\n",
      "Train Epoch: 70 [0/1649 (0%)]\tLoss: 0.269037\n",
      "\n",
      "Test set: Average loss: 0.5253, Accuracy: 342/412 (83%)\n",
      "\n",
      "Train Epoch: 71 [0/1649 (0%)]\tLoss: 0.624080\n",
      "\n",
      "Test set: Average loss: 0.4820, Accuracy: 350/412 (85%)\n",
      "\n",
      "Train Epoch: 72 [0/1649 (0%)]\tLoss: 0.355040\n",
      "\n",
      "Test set: Average loss: 0.4664, Accuracy: 351/412 (85%)\n",
      "\n",
      "Train Epoch: 73 [0/1649 (0%)]\tLoss: 0.650053\n",
      "\n",
      "Test set: Average loss: 0.4624, Accuracy: 348/412 (84%)\n",
      "\n",
      "Train Epoch: 74 [0/1649 (0%)]\tLoss: 0.477787\n",
      "\n",
      "Test set: Average loss: 0.4598, Accuracy: 351/412 (85%)\n",
      "\n",
      "Train Epoch: 75 [0/1649 (0%)]\tLoss: 0.399395\n",
      "\n",
      "Test set: Average loss: 0.4530, Accuracy: 352/412 (85%)\n",
      "\n",
      "Train Epoch: 76 [0/1649 (0%)]\tLoss: 0.564322\n",
      "\n",
      "Test set: Average loss: 0.4695, Accuracy: 351/412 (85%)\n",
      "\n",
      "Train Epoch: 77 [0/1649 (0%)]\tLoss: 0.290252\n",
      "\n",
      "Test set: Average loss: 0.4979, Accuracy: 350/412 (85%)\n",
      "\n",
      "Train Epoch: 78 [0/1649 (0%)]\tLoss: 0.608558\n",
      "\n",
      "Test set: Average loss: 0.4852, Accuracy: 346/412 (84%)\n",
      "\n",
      "Train Epoch: 79 [0/1649 (0%)]\tLoss: 0.479847\n",
      "\n",
      "Test set: Average loss: 0.4561, Accuracy: 355/412 (86%)\n",
      "\n",
      "Train Epoch: 80 [0/1649 (0%)]\tLoss: 0.482881\n",
      "\n",
      "Test set: Average loss: 0.4442, Accuracy: 356/412 (86%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 81 [0/1649 (0%)]\tLoss: 0.310351\n",
      "\n",
      "Test set: Average loss: 0.4424, Accuracy: 357/412 (87%)\n",
      "\n",
      "Train Epoch: 82 [0/1649 (0%)]\tLoss: 0.552178\n",
      "\n",
      "Test set: Average loss: 0.4280, Accuracy: 364/412 (88%)\n",
      "\n",
      "Train Epoch: 83 [0/1649 (0%)]\tLoss: 0.373719\n",
      "\n",
      "Test set: Average loss: 0.4262, Accuracy: 361/412 (88%)\n",
      "\n",
      "Train Epoch: 84 [0/1649 (0%)]\tLoss: 0.333602\n",
      "\n",
      "Test set: Average loss: 0.4400, Accuracy: 354/412 (86%)\n",
      "\n",
      "Train Epoch: 85 [0/1649 (0%)]\tLoss: 0.558345\n",
      "\n",
      "Test set: Average loss: 0.4365, Accuracy: 360/412 (87%)\n",
      "\n",
      "Train Epoch: 86 [0/1649 (0%)]\tLoss: 0.185171\n",
      "\n",
      "Test set: Average loss: 0.4311, Accuracy: 359/412 (87%)\n",
      "\n",
      "Train Epoch: 87 [0/1649 (0%)]\tLoss: 0.386502\n",
      "\n",
      "Test set: Average loss: 0.4203, Accuracy: 365/412 (89%)\n",
      "\n",
      "Train Epoch: 88 [0/1649 (0%)]\tLoss: 0.456538\n",
      "\n",
      "Test set: Average loss: 0.4162, Accuracy: 366/412 (89%)\n",
      "\n",
      "Train Epoch: 89 [0/1649 (0%)]\tLoss: 0.482534\n",
      "\n",
      "Test set: Average loss: 0.4200, Accuracy: 362/412 (88%)\n",
      "\n",
      "Train Epoch: 90 [0/1649 (0%)]\tLoss: 0.513574\n",
      "\n",
      "Test set: Average loss: 0.4055, Accuracy: 365/412 (89%)\n",
      "\n",
      "Train Epoch: 91 [0/1649 (0%)]\tLoss: 0.303699\n",
      "\n",
      "Test set: Average loss: 0.3980, Accuracy: 362/412 (88%)\n",
      "\n",
      "Train Epoch: 92 [0/1649 (0%)]\tLoss: 0.345034\n",
      "\n",
      "Test set: Average loss: 0.3979, Accuracy: 357/412 (87%)\n",
      "\n",
      "Train Epoch: 93 [0/1649 (0%)]\tLoss: 0.570337\n",
      "\n",
      "Test set: Average loss: 0.4116, Accuracy: 347/412 (84%)\n",
      "\n",
      "Train Epoch: 94 [0/1649 (0%)]\tLoss: 0.391019\n",
      "\n",
      "Test set: Average loss: 0.4115, Accuracy: 351/412 (85%)\n",
      "\n",
      "Train Epoch: 95 [0/1649 (0%)]\tLoss: 0.357564\n",
      "\n",
      "Test set: Average loss: 0.4049, Accuracy: 355/412 (86%)\n",
      "\n",
      "Train Epoch: 96 [0/1649 (0%)]\tLoss: 0.491096\n",
      "\n",
      "Test set: Average loss: 0.3847, Accuracy: 363/412 (88%)\n",
      "\n",
      "Train Epoch: 97 [0/1649 (0%)]\tLoss: 0.260759\n",
      "\n",
      "Test set: Average loss: 0.3827, Accuracy: 361/412 (88%)\n",
      "\n",
      "Train Epoch: 98 [0/1649 (0%)]\tLoss: 0.422758\n",
      "\n",
      "Test set: Average loss: 0.4032, Accuracy: 367/412 (89%)\n",
      "\n",
      "Train Epoch: 99 [0/1649 (0%)]\tLoss: 0.283232\n",
      "\n",
      "Test set: Average loss: 0.4358, Accuracy: 361/412 (88%)\n",
      "\n",
      "Train Epoch: 100 [0/1649 (0%)]\tLoss: 0.315606\n",
      "\n",
      "Test set: Average loss: 0.4133, Accuracy: 362/412 (88%)\n",
      "\n",
      "Train Epoch: 101 [0/1649 (0%)]\tLoss: 0.388820\n",
      "\n",
      "Test set: Average loss: 0.3874, Accuracy: 365/412 (89%)\n",
      "\n",
      "Train Epoch: 102 [0/1649 (0%)]\tLoss: 0.354698\n",
      "\n",
      "Test set: Average loss: 0.3540, Accuracy: 372/412 (90%)\n",
      "\n",
      "Train Epoch: 103 [0/1649 (0%)]\tLoss: 0.506739\n",
      "\n",
      "Test set: Average loss: 0.3678, Accuracy: 374/412 (91%)\n",
      "\n",
      "Train Epoch: 104 [0/1649 (0%)]\tLoss: 0.283575\n",
      "\n",
      "Test set: Average loss: 0.4142, Accuracy: 360/412 (87%)\n",
      "\n",
      "Train Epoch: 105 [0/1649 (0%)]\tLoss: 0.418030\n",
      "\n",
      "Test set: Average loss: 0.3983, Accuracy: 362/412 (88%)\n",
      "\n",
      "Train Epoch: 106 [0/1649 (0%)]\tLoss: 0.448222\n",
      "\n",
      "Test set: Average loss: 0.3594, Accuracy: 367/412 (89%)\n",
      "\n",
      "Train Epoch: 107 [0/1649 (0%)]\tLoss: 0.334215\n",
      "\n",
      "Test set: Average loss: 0.3374, Accuracy: 373/412 (91%)\n",
      "\n",
      "Train Epoch: 108 [0/1649 (0%)]\tLoss: 0.419947\n",
      "\n",
      "Test set: Average loss: 0.3404, Accuracy: 368/412 (89%)\n",
      "\n",
      "Train Epoch: 109 [0/1649 (0%)]\tLoss: 0.468816\n",
      "\n",
      "Test set: Average loss: 0.3698, Accuracy: 360/412 (87%)\n",
      "\n",
      "Train Epoch: 110 [0/1649 (0%)]\tLoss: 0.503250\n",
      "\n",
      "Test set: Average loss: 0.4012, Accuracy: 357/412 (87%)\n",
      "\n",
      "Train Epoch: 111 [0/1649 (0%)]\tLoss: 0.320294\n",
      "\n",
      "Test set: Average loss: 0.4226, Accuracy: 353/412 (86%)\n",
      "\n",
      "Train Epoch: 112 [0/1649 (0%)]\tLoss: 0.648745\n",
      "\n",
      "Test set: Average loss: 0.3965, Accuracy: 358/412 (87%)\n",
      "\n",
      "Train Epoch: 113 [0/1649 (0%)]\tLoss: 0.248175\n",
      "\n",
      "Test set: Average loss: 0.3569, Accuracy: 369/412 (90%)\n",
      "\n",
      "Train Epoch: 114 [0/1649 (0%)]\tLoss: 0.299666\n",
      "\n",
      "Test set: Average loss: 0.3332, Accuracy: 373/412 (91%)\n",
      "\n",
      "Train Epoch: 115 [0/1649 (0%)]\tLoss: 0.353987\n",
      "\n",
      "Test set: Average loss: 0.3252, Accuracy: 376/412 (91%)\n",
      "\n",
      "Train Epoch: 116 [0/1649 (0%)]\tLoss: 0.252242\n",
      "\n",
      "Test set: Average loss: 0.3327, Accuracy: 376/412 (91%)\n",
      "\n",
      "Train Epoch: 117 [0/1649 (0%)]\tLoss: 0.285139\n",
      "\n",
      "Test set: Average loss: 0.3427, Accuracy: 376/412 (91%)\n",
      "\n",
      "Train Epoch: 118 [0/1649 (0%)]\tLoss: 0.324298\n",
      "\n",
      "Test set: Average loss: 0.3416, Accuracy: 377/412 (92%)\n",
      "\n",
      "Train Epoch: 119 [0/1649 (0%)]\tLoss: 0.375438\n",
      "\n",
      "Test set: Average loss: 0.3394, Accuracy: 374/412 (91%)\n",
      "\n",
      "Train Epoch: 120 [0/1649 (0%)]\tLoss: 0.255247\n",
      "\n",
      "Test set: Average loss: 0.3266, Accuracy: 377/412 (92%)\n",
      "\n",
      "Train Epoch: 121 [0/1649 (0%)]\tLoss: 0.269170\n",
      "\n",
      "Test set: Average loss: 0.3175, Accuracy: 378/412 (92%)\n",
      "\n",
      "Train Epoch: 122 [0/1649 (0%)]\tLoss: 0.157038\n",
      "\n",
      "Test set: Average loss: 0.3107, Accuracy: 377/412 (92%)\n",
      "\n",
      "Train Epoch: 123 [0/1649 (0%)]\tLoss: 0.194856\n",
      "\n",
      "Test set: Average loss: 0.3113, Accuracy: 376/412 (91%)\n",
      "\n",
      "Train Epoch: 124 [0/1649 (0%)]\tLoss: 0.234391\n",
      "\n",
      "Test set: Average loss: 0.3159, Accuracy: 373/412 (91%)\n",
      "\n",
      "Train Epoch: 125 [0/1649 (0%)]\tLoss: 0.317051\n",
      "\n",
      "Test set: Average loss: 0.3142, Accuracy: 375/412 (91%)\n",
      "\n",
      "Train Epoch: 126 [0/1649 (0%)]\tLoss: 0.231010\n",
      "\n",
      "Test set: Average loss: 0.3153, Accuracy: 376/412 (91%)\n",
      "\n",
      "Train Epoch: 127 [0/1649 (0%)]\tLoss: 0.196840\n",
      "\n",
      "Test set: Average loss: 0.3188, Accuracy: 379/412 (92%)\n",
      "\n",
      "Train Epoch: 128 [0/1649 (0%)]\tLoss: 0.392495\n",
      "\n",
      "Test set: Average loss: 0.3177, Accuracy: 375/412 (91%)\n",
      "\n",
      "Train Epoch: 129 [0/1649 (0%)]\tLoss: 0.230954\n",
      "\n",
      "Test set: Average loss: 0.3145, Accuracy: 375/412 (91%)\n",
      "\n",
      "Train Epoch: 130 [0/1649 (0%)]\tLoss: 0.267000\n",
      "\n",
      "Test set: Average loss: 0.3015, Accuracy: 375/412 (91%)\n",
      "\n",
      "Train Epoch: 131 [0/1649 (0%)]\tLoss: 0.171751\n",
      "\n",
      "Test set: Average loss: 0.2988, Accuracy: 376/412 (91%)\n",
      "\n",
      "Train Epoch: 132 [0/1649 (0%)]\tLoss: 0.145332\n",
      "\n",
      "Test set: Average loss: 0.2858, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 133 [0/1649 (0%)]\tLoss: 0.212526\n",
      "\n",
      "Test set: Average loss: 0.2791, Accuracy: 384/412 (93%)\n",
      "\n",
      "Train Epoch: 134 [0/1649 (0%)]\tLoss: 0.231515\n",
      "\n",
      "Test set: Average loss: 0.2771, Accuracy: 379/412 (92%)\n",
      "\n",
      "Train Epoch: 135 [0/1649 (0%)]\tLoss: 0.176505\n",
      "\n",
      "Test set: Average loss: 0.2588, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 136 [0/1649 (0%)]\tLoss: 0.078742\n",
      "\n",
      "Test set: Average loss: 0.2490, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 137 [0/1649 (0%)]\tLoss: 0.329711\n",
      "\n",
      "Test set: Average loss: 0.2413, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 138 [0/1649 (0%)]\tLoss: 0.137068\n",
      "\n",
      "Test set: Average loss: 0.2504, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 139 [0/1649 (0%)]\tLoss: 0.199823\n",
      "\n",
      "Test set: Average loss: 0.2521, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 140 [0/1649 (0%)]\tLoss: 0.273690\n",
      "\n",
      "Test set: Average loss: 0.2526, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 141 [0/1649 (0%)]\tLoss: 0.263184\n",
      "\n",
      "Test set: Average loss: 0.2513, Accuracy: 384/412 (93%)\n",
      "\n",
      "Train Epoch: 142 [0/1649 (0%)]\tLoss: 0.224636\n",
      "\n",
      "Test set: Average loss: 0.2394, Accuracy: 383/412 (93%)\n",
      "\n",
      "Train Epoch: 143 [0/1649 (0%)]\tLoss: 0.206736\n",
      "\n",
      "Test set: Average loss: 0.2370, Accuracy: 384/412 (93%)\n",
      "\n",
      "Train Epoch: 144 [0/1649 (0%)]\tLoss: 0.136703\n",
      "\n",
      "Test set: Average loss: 0.2303, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 145 [0/1649 (0%)]\tLoss: 0.135510\n",
      "\n",
      "Test set: Average loss: 0.2296, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 146 [0/1649 (0%)]\tLoss: 0.082124\n",
      "\n",
      "Test set: Average loss: 0.2321, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 147 [0/1649 (0%)]\tLoss: 0.194147\n",
      "\n",
      "Test set: Average loss: 0.2352, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 148 [0/1649 (0%)]\tLoss: 0.180635\n",
      "\n",
      "Test set: Average loss: 0.2275, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 149 [0/1649 (0%)]\tLoss: 0.098591\n",
      "\n",
      "Test set: Average loss: 0.2246, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 150 [0/1649 (0%)]\tLoss: 0.141290\n",
      "\n",
      "Test set: Average loss: 0.2204, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 151 [0/1649 (0%)]\tLoss: 0.228745\n",
      "\n",
      "Test set: Average loss: 0.2123, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 152 [0/1649 (0%)]\tLoss: 0.101509\n",
      "\n",
      "Test set: Average loss: 0.2064, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 153 [0/1649 (0%)]\tLoss: 0.140874\n",
      "\n",
      "Test set: Average loss: 0.2077, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 154 [0/1649 (0%)]\tLoss: 0.149151\n",
      "\n",
      "Test set: Average loss: 0.2133, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 155 [0/1649 (0%)]\tLoss: 0.114925\n",
      "\n",
      "Test set: Average loss: 0.2172, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 156 [0/1649 (0%)]\tLoss: 0.135397\n",
      "\n",
      "Test set: Average loss: 0.2169, Accuracy: 384/412 (93%)\n",
      "\n",
      "Train Epoch: 157 [0/1649 (0%)]\tLoss: 0.192000\n",
      "\n",
      "Test set: Average loss: 0.2144, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 158 [0/1649 (0%)]\tLoss: 0.130035\n",
      "\n",
      "Test set: Average loss: 0.2173, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 159 [0/1649 (0%)]\tLoss: 0.156642\n",
      "\n",
      "Test set: Average loss: 0.2293, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 160 [0/1649 (0%)]\tLoss: 0.181610\n",
      "\n",
      "Test set: Average loss: 0.2422, Accuracy: 383/412 (93%)\n",
      "\n",
      "Train Epoch: 161 [0/1649 (0%)]\tLoss: 0.269824\n",
      "\n",
      "Test set: Average loss: 0.2455, Accuracy: 383/412 (93%)\n",
      "\n",
      "Train Epoch: 162 [0/1649 (0%)]\tLoss: 0.090073\n",
      "\n",
      "Test set: Average loss: 0.2456, Accuracy: 386/412 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 163 [0/1649 (0%)]\tLoss: 0.389226\n",
      "\n",
      "Test set: Average loss: 0.2290, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 164 [0/1649 (0%)]\tLoss: 0.071665\n",
      "\n",
      "Test set: Average loss: 0.2254, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 165 [0/1649 (0%)]\tLoss: 0.073361\n",
      "\n",
      "Test set: Average loss: 0.2314, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 166 [0/1649 (0%)]\tLoss: 0.094039\n",
      "\n",
      "Test set: Average loss: 0.2354, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 167 [0/1649 (0%)]\tLoss: 0.208750\n",
      "\n",
      "Test set: Average loss: 0.2368, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 168 [0/1649 (0%)]\tLoss: 0.104028\n",
      "\n",
      "Test set: Average loss: 0.2408, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 169 [0/1649 (0%)]\tLoss: 0.100577\n",
      "\n",
      "Test set: Average loss: 0.2371, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 170 [0/1649 (0%)]\tLoss: 0.193082\n",
      "\n",
      "Test set: Average loss: 0.2299, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 171 [0/1649 (0%)]\tLoss: 0.116927\n",
      "\n",
      "Test set: Average loss: 0.2250, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 172 [0/1649 (0%)]\tLoss: 0.093244\n",
      "\n",
      "Test set: Average loss: 0.2218, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 173 [0/1649 (0%)]\tLoss: 0.153306\n",
      "\n",
      "Test set: Average loss: 0.2218, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 174 [0/1649 (0%)]\tLoss: 0.083005\n",
      "\n",
      "Test set: Average loss: 0.2269, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 175 [0/1649 (0%)]\tLoss: 0.190262\n",
      "\n",
      "Test set: Average loss: 0.2282, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 176 [0/1649 (0%)]\tLoss: 0.290992\n",
      "\n",
      "Test set: Average loss: 0.2296, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 177 [0/1649 (0%)]\tLoss: 0.067898\n",
      "\n",
      "Test set: Average loss: 0.2371, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 178 [0/1649 (0%)]\tLoss: 0.152055\n",
      "\n",
      "Test set: Average loss: 0.2404, Accuracy: 384/412 (93%)\n",
      "\n",
      "Train Epoch: 179 [0/1649 (0%)]\tLoss: 0.105940\n",
      "\n",
      "Test set: Average loss: 0.2440, Accuracy: 382/412 (93%)\n",
      "\n",
      "Train Epoch: 180 [0/1649 (0%)]\tLoss: 0.119522\n",
      "\n",
      "Test set: Average loss: 0.2499, Accuracy: 382/412 (93%)\n",
      "\n",
      "Train Epoch: 181 [0/1649 (0%)]\tLoss: 0.182037\n",
      "\n",
      "Test set: Average loss: 0.2420, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 182 [0/1649 (0%)]\tLoss: 0.131325\n",
      "\n",
      "Test set: Average loss: 0.2367, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 183 [0/1649 (0%)]\tLoss: 0.034683\n",
      "\n",
      "Test set: Average loss: 0.2353, Accuracy: 384/412 (93%)\n",
      "\n",
      "Train Epoch: 184 [0/1649 (0%)]\tLoss: 0.090384\n",
      "\n",
      "Test set: Average loss: 0.2392, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 185 [0/1649 (0%)]\tLoss: 0.085996\n",
      "\n",
      "Test set: Average loss: 0.2430, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 186 [0/1649 (0%)]\tLoss: 0.168973\n",
      "\n",
      "Test set: Average loss: 0.2425, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 187 [0/1649 (0%)]\tLoss: 0.133585\n",
      "\n",
      "Test set: Average loss: 0.2413, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 188 [0/1649 (0%)]\tLoss: 0.118754\n",
      "\n",
      "Test set: Average loss: 0.2396, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 189 [0/1649 (0%)]\tLoss: 0.046992\n",
      "\n",
      "Test set: Average loss: 0.2418, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 190 [0/1649 (0%)]\tLoss: 0.342995\n",
      "\n",
      "Test set: Average loss: 0.2369, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 191 [0/1649 (0%)]\tLoss: 0.194289\n",
      "\n",
      "Test set: Average loss: 0.2278, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 192 [0/1649 (0%)]\tLoss: 0.141576\n",
      "\n",
      "Test set: Average loss: 0.2202, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 193 [0/1649 (0%)]\tLoss: 0.090092\n",
      "\n",
      "Test set: Average loss: 0.2236, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 194 [0/1649 (0%)]\tLoss: 0.200387\n",
      "\n",
      "Test set: Average loss: 0.2247, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 195 [0/1649 (0%)]\tLoss: 0.263413\n",
      "\n",
      "Test set: Average loss: 0.2135, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 196 [0/1649 (0%)]\tLoss: 0.062259\n",
      "\n",
      "Test set: Average loss: 0.2074, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 197 [0/1649 (0%)]\tLoss: 0.140714\n",
      "\n",
      "Test set: Average loss: 0.2039, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 198 [0/1649 (0%)]\tLoss: 0.056007\n",
      "\n",
      "Test set: Average loss: 0.2020, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 199 [0/1649 (0%)]\tLoss: 0.166217\n",
      "\n",
      "Test set: Average loss: 0.1951, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 200 [0/1649 (0%)]\tLoss: 0.174992\n",
      "\n",
      "Test set: Average loss: 0.1903, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 201 [0/1649 (0%)]\tLoss: 0.071979\n",
      "\n",
      "Test set: Average loss: 0.1892, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 202 [0/1649 (0%)]\tLoss: 0.202376\n",
      "\n",
      "Test set: Average loss: 0.1917, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 203 [0/1649 (0%)]\tLoss: 0.085544\n",
      "\n",
      "Test set: Average loss: 0.1915, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 204 [0/1649 (0%)]\tLoss: 0.143143\n",
      "\n",
      "Test set: Average loss: 0.1943, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 205 [0/1649 (0%)]\tLoss: 0.230120\n",
      "\n",
      "Test set: Average loss: 0.2020, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 206 [0/1649 (0%)]\tLoss: 0.150752\n",
      "\n",
      "Test set: Average loss: 0.2078, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 207 [0/1649 (0%)]\tLoss: 0.113650\n",
      "\n",
      "Test set: Average loss: 0.2222, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 208 [0/1649 (0%)]\tLoss: 0.187112\n",
      "\n",
      "Test set: Average loss: 0.2118, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 209 [0/1649 (0%)]\tLoss: 0.180118\n",
      "\n",
      "Test set: Average loss: 0.1909, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 210 [0/1649 (0%)]\tLoss: 0.055637\n",
      "\n",
      "Test set: Average loss: 0.1815, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 211 [0/1649 (0%)]\tLoss: 0.123361\n",
      "\n",
      "Test set: Average loss: 0.1769, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 212 [0/1649 (0%)]\tLoss: 0.164533\n",
      "\n",
      "Test set: Average loss: 0.1844, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 213 [0/1649 (0%)]\tLoss: 0.197451\n",
      "\n",
      "Test set: Average loss: 0.2292, Accuracy: 382/412 (93%)\n",
      "\n",
      "Train Epoch: 214 [0/1649 (0%)]\tLoss: 0.129744\n",
      "\n",
      "Test set: Average loss: 0.2354, Accuracy: 380/412 (92%)\n",
      "\n",
      "Train Epoch: 215 [0/1649 (0%)]\tLoss: 0.173261\n",
      "\n",
      "Test set: Average loss: 0.2074, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 216 [0/1649 (0%)]\tLoss: 0.084211\n",
      "\n",
      "Test set: Average loss: 0.1869, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 217 [0/1649 (0%)]\tLoss: 0.239264\n",
      "\n",
      "Test set: Average loss: 0.1764, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 218 [0/1649 (0%)]\tLoss: 0.130607\n",
      "\n",
      "Test set: Average loss: 0.1750, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 219 [0/1649 (0%)]\tLoss: 0.135188\n",
      "\n",
      "Test set: Average loss: 0.1817, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 220 [0/1649 (0%)]\tLoss: 0.105738\n",
      "\n",
      "Test set: Average loss: 0.1869, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 221 [0/1649 (0%)]\tLoss: 0.086251\n",
      "\n",
      "Test set: Average loss: 0.1926, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 222 [0/1649 (0%)]\tLoss: 0.125654\n",
      "\n",
      "Test set: Average loss: 0.1867, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 223 [0/1649 (0%)]\tLoss: 0.110441\n",
      "\n",
      "Test set: Average loss: 0.1875, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 224 [0/1649 (0%)]\tLoss: 0.071356\n",
      "\n",
      "Test set: Average loss: 0.1955, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 225 [0/1649 (0%)]\tLoss: 0.099145\n",
      "\n",
      "Test set: Average loss: 0.2011, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 226 [0/1649 (0%)]\tLoss: 0.053409\n",
      "\n",
      "Test set: Average loss: 0.2049, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 227 [0/1649 (0%)]\tLoss: 0.226615\n",
      "\n",
      "Test set: Average loss: 0.1941, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 228 [0/1649 (0%)]\tLoss: 0.140224\n",
      "\n",
      "Test set: Average loss: 0.1925, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 229 [0/1649 (0%)]\tLoss: 0.129372\n",
      "\n",
      "Test set: Average loss: 0.1978, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 230 [0/1649 (0%)]\tLoss: 0.099593\n",
      "\n",
      "Test set: Average loss: 0.2055, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 231 [0/1649 (0%)]\tLoss: 0.180316\n",
      "\n",
      "Test set: Average loss: 0.2152, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 232 [0/1649 (0%)]\tLoss: 0.027351\n",
      "\n",
      "Test set: Average loss: 0.2296, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 233 [0/1649 (0%)]\tLoss: 0.157648\n",
      "\n",
      "Test set: Average loss: 0.2253, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 234 [0/1649 (0%)]\tLoss: 0.044434\n",
      "\n",
      "Test set: Average loss: 0.2219, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 235 [0/1649 (0%)]\tLoss: 0.047969\n",
      "\n",
      "Test set: Average loss: 0.2231, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 236 [0/1649 (0%)]\tLoss: 0.046323\n",
      "\n",
      "Test set: Average loss: 0.2284, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 237 [0/1649 (0%)]\tLoss: 0.055400\n",
      "\n",
      "Test set: Average loss: 0.2291, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 238 [0/1649 (0%)]\tLoss: 0.045411\n",
      "\n",
      "Test set: Average loss: 0.2296, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 239 [0/1649 (0%)]\tLoss: 0.158245\n",
      "\n",
      "Test set: Average loss: 0.2231, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 240 [0/1649 (0%)]\tLoss: 0.120973\n",
      "\n",
      "Test set: Average loss: 0.2205, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 241 [0/1649 (0%)]\tLoss: 0.124087\n",
      "\n",
      "Test set: Average loss: 0.2225, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 242 [0/1649 (0%)]\tLoss: 0.060633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2349, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 243 [0/1649 (0%)]\tLoss: 0.073105\n",
      "\n",
      "Test set: Average loss: 0.2439, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 244 [0/1649 (0%)]\tLoss: 0.109655\n",
      "\n",
      "Test set: Average loss: 0.2279, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 245 [0/1649 (0%)]\tLoss: 0.198813\n",
      "\n",
      "Test set: Average loss: 0.2114, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 246 [0/1649 (0%)]\tLoss: 0.029848\n",
      "\n",
      "Test set: Average loss: 0.2090, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 247 [0/1649 (0%)]\tLoss: 0.071174\n",
      "\n",
      "Test set: Average loss: 0.2148, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 248 [0/1649 (0%)]\tLoss: 0.074284\n",
      "\n",
      "Test set: Average loss: 0.2248, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 249 [0/1649 (0%)]\tLoss: 0.267024\n",
      "\n",
      "Test set: Average loss: 0.2194, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 250 [0/1649 (0%)]\tLoss: 0.089586\n",
      "\n",
      "Test set: Average loss: 0.2151, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 251 [0/1649 (0%)]\tLoss: 0.173216\n",
      "\n",
      "Test set: Average loss: 0.2161, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 252 [0/1649 (0%)]\tLoss: 0.019006\n",
      "\n",
      "Test set: Average loss: 0.2245, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 253 [0/1649 (0%)]\tLoss: 0.057968\n",
      "\n",
      "Test set: Average loss: 0.2379, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 254 [0/1649 (0%)]\tLoss: 0.057183\n",
      "\n",
      "Test set: Average loss: 0.2493, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 255 [0/1649 (0%)]\tLoss: 0.092830\n",
      "\n",
      "Test set: Average loss: 0.2476, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 256 [0/1649 (0%)]\tLoss: 0.132457\n",
      "\n",
      "Test set: Average loss: 0.2459, Accuracy: 382/412 (93%)\n",
      "\n",
      "Train Epoch: 257 [0/1649 (0%)]\tLoss: 0.161058\n",
      "\n",
      "Test set: Average loss: 0.2498, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 258 [0/1649 (0%)]\tLoss: 0.124184\n",
      "\n",
      "Test set: Average loss: 0.2531, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 259 [0/1649 (0%)]\tLoss: 0.077536\n",
      "\n",
      "Test set: Average loss: 0.2500, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 260 [0/1649 (0%)]\tLoss: 0.126399\n",
      "\n",
      "Test set: Average loss: 0.2396, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 261 [0/1649 (0%)]\tLoss: 0.075826\n",
      "\n",
      "Test set: Average loss: 0.2282, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 262 [0/1649 (0%)]\tLoss: 0.028534\n",
      "\n",
      "Test set: Average loss: 0.2183, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 263 [0/1649 (0%)]\tLoss: 0.008037\n",
      "\n",
      "Test set: Average loss: 0.2129, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 264 [0/1649 (0%)]\tLoss: 0.037937\n",
      "\n",
      "Test set: Average loss: 0.2057, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 265 [0/1649 (0%)]\tLoss: 0.035579\n",
      "\n",
      "Test set: Average loss: 0.2023, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 266 [0/1649 (0%)]\tLoss: 0.030327\n",
      "\n",
      "Test set: Average loss: 0.2052, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 267 [0/1649 (0%)]\tLoss: 0.078286\n",
      "\n",
      "Test set: Average loss: 0.2111, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 268 [0/1649 (0%)]\tLoss: 0.050962\n",
      "\n",
      "Test set: Average loss: 0.2134, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 269 [0/1649 (0%)]\tLoss: 0.039772\n",
      "\n",
      "Test set: Average loss: 0.2142, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 270 [0/1649 (0%)]\tLoss: 0.052276\n",
      "\n",
      "Test set: Average loss: 0.2110, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 271 [0/1649 (0%)]\tLoss: 0.174045\n",
      "\n",
      "Test set: Average loss: 0.1884, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 272 [0/1649 (0%)]\tLoss: 0.078556\n",
      "\n",
      "Test set: Average loss: 0.1843, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 273 [0/1649 (0%)]\tLoss: 0.010668\n",
      "\n",
      "Test set: Average loss: 0.1889, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 274 [0/1649 (0%)]\tLoss: 0.055972\n",
      "\n",
      "Test set: Average loss: 0.2046, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 275 [0/1649 (0%)]\tLoss: 0.089114\n",
      "\n",
      "Test set: Average loss: 0.2282, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 276 [0/1649 (0%)]\tLoss: 0.027216\n",
      "\n",
      "Test set: Average loss: 0.2551, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 277 [0/1649 (0%)]\tLoss: 0.024388\n",
      "\n",
      "Test set: Average loss: 0.2791, Accuracy: 380/412 (92%)\n",
      "\n",
      "Train Epoch: 278 [0/1649 (0%)]\tLoss: 0.168837\n",
      "\n",
      "Test set: Average loss: 0.2568, Accuracy: 383/412 (93%)\n",
      "\n",
      "Train Epoch: 279 [0/1649 (0%)]\tLoss: 0.244391\n",
      "\n",
      "Test set: Average loss: 0.2312, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 280 [0/1649 (0%)]\tLoss: 0.123733\n",
      "\n",
      "Test set: Average loss: 0.2382, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 281 [0/1649 (0%)]\tLoss: 0.012862\n",
      "\n",
      "Test set: Average loss: 0.2760, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 282 [0/1649 (0%)]\tLoss: 0.125112\n",
      "\n",
      "Test set: Average loss: 0.2739, Accuracy: 381/412 (92%)\n",
      "\n",
      "Train Epoch: 283 [0/1649 (0%)]\tLoss: 0.074220\n",
      "\n",
      "Test set: Average loss: 0.2346, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 284 [0/1649 (0%)]\tLoss: 0.046139\n",
      "\n",
      "Test set: Average loss: 0.1989, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 285 [0/1649 (0%)]\tLoss: 0.030532\n",
      "\n",
      "Test set: Average loss: 0.1873, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 286 [0/1649 (0%)]\tLoss: 0.149075\n",
      "\n",
      "Test set: Average loss: 0.1890, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 287 [0/1649 (0%)]\tLoss: 0.056308\n",
      "\n",
      "Test set: Average loss: 0.1957, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 288 [0/1649 (0%)]\tLoss: 0.028365\n",
      "\n",
      "Test set: Average loss: 0.2062, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 289 [0/1649 (0%)]\tLoss: 0.141085\n",
      "\n",
      "Test set: Average loss: 0.2051, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 290 [0/1649 (0%)]\tLoss: 0.143708\n",
      "\n",
      "Test set: Average loss: 0.2165, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 291 [0/1649 (0%)]\tLoss: 0.030446\n",
      "\n",
      "Test set: Average loss: 0.2336, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 292 [0/1649 (0%)]\tLoss: 0.083973\n",
      "\n",
      "Test set: Average loss: 0.2373, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 293 [0/1649 (0%)]\tLoss: 0.025105\n",
      "\n",
      "Test set: Average loss: 0.2364, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 294 [0/1649 (0%)]\tLoss: 0.023628\n",
      "\n",
      "Test set: Average loss: 0.2312, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 295 [0/1649 (0%)]\tLoss: 0.105541\n",
      "\n",
      "Test set: Average loss: 0.2220, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 296 [0/1649 (0%)]\tLoss: 0.012722\n",
      "\n",
      "Test set: Average loss: 0.2177, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 297 [0/1649 (0%)]\tLoss: 0.036504\n",
      "\n",
      "Test set: Average loss: 0.2166, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 298 [0/1649 (0%)]\tLoss: 0.033972\n",
      "\n",
      "Test set: Average loss: 0.2172, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 299 [0/1649 (0%)]\tLoss: 0.021728\n",
      "\n",
      "Test set: Average loss: 0.2180, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 300 [0/1649 (0%)]\tLoss: 0.046661\n",
      "\n",
      "Test set: Average loss: 0.2184, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 301 [0/1649 (0%)]\tLoss: 0.096019\n",
      "\n",
      "Test set: Average loss: 0.2196, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 302 [0/1649 (0%)]\tLoss: 0.041485\n",
      "\n",
      "Test set: Average loss: 0.2214, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 303 [0/1649 (0%)]\tLoss: 0.036697\n",
      "\n",
      "Test set: Average loss: 0.2273, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 304 [0/1649 (0%)]\tLoss: 0.024587\n",
      "\n",
      "Test set: Average loss: 0.2370, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 305 [0/1649 (0%)]\tLoss: 0.158458\n",
      "\n",
      "Test set: Average loss: 0.2245, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 306 [0/1649 (0%)]\tLoss: 0.087053\n",
      "\n",
      "Test set: Average loss: 0.2163, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 307 [0/1649 (0%)]\tLoss: 0.068223\n",
      "\n",
      "Test set: Average loss: 0.2100, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 308 [0/1649 (0%)]\tLoss: 0.085339\n",
      "\n",
      "Test set: Average loss: 0.2070, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 309 [0/1649 (0%)]\tLoss: 0.097974\n",
      "\n",
      "Test set: Average loss: 0.2012, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 310 [0/1649 (0%)]\tLoss: 0.068744\n",
      "\n",
      "Test set: Average loss: 0.1956, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 311 [0/1649 (0%)]\tLoss: 0.126471\n",
      "\n",
      "Test set: Average loss: 0.1950, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 312 [0/1649 (0%)]\tLoss: 0.028891\n",
      "\n",
      "Test set: Average loss: 0.1952, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 313 [0/1649 (0%)]\tLoss: 0.088086\n",
      "\n",
      "Test set: Average loss: 0.1931, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 314 [0/1649 (0%)]\tLoss: 0.032871\n",
      "\n",
      "Test set: Average loss: 0.1967, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 315 [0/1649 (0%)]\tLoss: 0.018164\n",
      "\n",
      "Test set: Average loss: 0.2034, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 316 [0/1649 (0%)]\tLoss: 0.066109\n",
      "\n",
      "Test set: Average loss: 0.2098, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 317 [0/1649 (0%)]\tLoss: 0.047492\n",
      "\n",
      "Test set: Average loss: 0.2147, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 318 [0/1649 (0%)]\tLoss: 0.010458\n",
      "\n",
      "Test set: Average loss: 0.2197, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 319 [0/1649 (0%)]\tLoss: 0.058638\n",
      "\n",
      "Test set: Average loss: 0.2244, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 320 [0/1649 (0%)]\tLoss: 0.009924\n",
      "\n",
      "Test set: Average loss: 0.2311, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 321 [0/1649 (0%)]\tLoss: 0.045184\n",
      "\n",
      "Test set: Average loss: 0.2300, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 322 [0/1649 (0%)]\tLoss: 0.017388\n",
      "\n",
      "Test set: Average loss: 0.2289, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 323 [0/1649 (0%)]\tLoss: 0.098726\n",
      "\n",
      "Test set: Average loss: 0.2205, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 324 [0/1649 (0%)]\tLoss: 0.075585\n",
      "\n",
      "Test set: Average loss: 0.2226, Accuracy: 392/412 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 325 [0/1649 (0%)]\tLoss: 0.068811\n",
      "\n",
      "Test set: Average loss: 0.2306, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 326 [0/1649 (0%)]\tLoss: 0.029743\n",
      "\n",
      "Test set: Average loss: 0.2409, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 327 [0/1649 (0%)]\tLoss: 0.012587\n",
      "\n",
      "Test set: Average loss: 0.2542, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 328 [0/1649 (0%)]\tLoss: 0.091824\n",
      "\n",
      "Test set: Average loss: 0.2693, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 329 [0/1649 (0%)]\tLoss: 0.157329\n",
      "\n",
      "Test set: Average loss: 0.2483, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 330 [0/1649 (0%)]\tLoss: 0.089605\n",
      "\n",
      "Test set: Average loss: 0.2241, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 331 [0/1649 (0%)]\tLoss: 0.027158\n",
      "\n",
      "Test set: Average loss: 0.2195, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 332 [0/1649 (0%)]\tLoss: 0.036038\n",
      "\n",
      "Test set: Average loss: 0.2273, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 333 [0/1649 (0%)]\tLoss: 0.003001\n",
      "\n",
      "Test set: Average loss: 0.2449, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 334 [0/1649 (0%)]\tLoss: 0.043181\n",
      "\n",
      "Test set: Average loss: 0.2580, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 335 [0/1649 (0%)]\tLoss: 0.105294\n",
      "\n",
      "Test set: Average loss: 0.2783, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 336 [0/1649 (0%)]\tLoss: 0.090307\n",
      "\n",
      "Test set: Average loss: 0.2968, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 337 [0/1649 (0%)]\tLoss: 0.105363\n",
      "\n",
      "Test set: Average loss: 0.3005, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 338 [0/1649 (0%)]\tLoss: 0.198604\n",
      "\n",
      "Test set: Average loss: 0.2358, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 339 [0/1649 (0%)]\tLoss: 0.062679\n",
      "\n",
      "Test set: Average loss: 0.2266, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 340 [0/1649 (0%)]\tLoss: 0.058717\n",
      "\n",
      "Test set: Average loss: 0.2353, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 341 [0/1649 (0%)]\tLoss: 0.126093\n",
      "\n",
      "Test set: Average loss: 0.2388, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 342 [0/1649 (0%)]\tLoss: 0.070850\n",
      "\n",
      "Test set: Average loss: 0.2446, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 343 [0/1649 (0%)]\tLoss: 0.075829\n",
      "\n",
      "Test set: Average loss: 0.2513, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 344 [0/1649 (0%)]\tLoss: 0.040978\n",
      "\n",
      "Test set: Average loss: 0.2513, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 345 [0/1649 (0%)]\tLoss: 0.013978\n",
      "\n",
      "Test set: Average loss: 0.2523, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 346 [0/1649 (0%)]\tLoss: 0.126933\n",
      "\n",
      "Test set: Average loss: 0.2416, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 347 [0/1649 (0%)]\tLoss: 0.143651\n",
      "\n",
      "Test set: Average loss: 0.2367, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 348 [0/1649 (0%)]\tLoss: 0.003264\n",
      "\n",
      "Test set: Average loss: 0.2504, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 349 [0/1649 (0%)]\tLoss: 0.012805\n",
      "\n",
      "Test set: Average loss: 0.2785, Accuracy: 386/412 (94%)\n",
      "\n",
      "Train Epoch: 350 [0/1649 (0%)]\tLoss: 0.069923\n",
      "\n",
      "Test set: Average loss: 0.2866, Accuracy: 383/412 (93%)\n",
      "\n",
      "Train Epoch: 351 [0/1649 (0%)]\tLoss: 0.185915\n",
      "\n",
      "Test set: Average loss: 0.2435, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 352 [0/1649 (0%)]\tLoss: 0.140949\n",
      "\n",
      "Test set: Average loss: 0.2113, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 353 [0/1649 (0%)]\tLoss: 0.006342\n",
      "\n",
      "Test set: Average loss: 0.2018, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 354 [0/1649 (0%)]\tLoss: 0.049833\n",
      "\n",
      "Test set: Average loss: 0.2024, Accuracy: 399/412 (97%)\n",
      "\n",
      "Train Epoch: 355 [0/1649 (0%)]\tLoss: 0.007636\n",
      "\n",
      "Test set: Average loss: 0.2118, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 356 [0/1649 (0%)]\tLoss: 0.022166\n",
      "\n",
      "Test set: Average loss: 0.2251, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 357 [0/1649 (0%)]\tLoss: 0.027031\n",
      "\n",
      "Test set: Average loss: 0.2339, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 358 [0/1649 (0%)]\tLoss: 0.092520\n",
      "\n",
      "Test set: Average loss: 0.2361, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 359 [0/1649 (0%)]\tLoss: 0.026205\n",
      "\n",
      "Test set: Average loss: 0.2357, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 360 [0/1649 (0%)]\tLoss: 0.037461\n",
      "\n",
      "Test set: Average loss: 0.2280, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 361 [0/1649 (0%)]\tLoss: 0.074235\n",
      "\n",
      "Test set: Average loss: 0.2192, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 362 [0/1649 (0%)]\tLoss: 0.158843\n",
      "\n",
      "Test set: Average loss: 0.2288, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 363 [0/1649 (0%)]\tLoss: 0.084273\n",
      "\n",
      "Test set: Average loss: 0.2350, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 364 [0/1649 (0%)]\tLoss: 0.014666\n",
      "\n",
      "Test set: Average loss: 0.2472, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 365 [0/1649 (0%)]\tLoss: 0.021378\n",
      "\n",
      "Test set: Average loss: 0.2570, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 366 [0/1649 (0%)]\tLoss: 0.099189\n",
      "\n",
      "Test set: Average loss: 0.2513, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 367 [0/1649 (0%)]\tLoss: 0.024131\n",
      "\n",
      "Test set: Average loss: 0.2546, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 368 [0/1649 (0%)]\tLoss: 0.057171\n",
      "\n",
      "Test set: Average loss: 0.2586, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 369 [0/1649 (0%)]\tLoss: 0.038636\n",
      "\n",
      "Test set: Average loss: 0.2661, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 370 [0/1649 (0%)]\tLoss: 0.221459\n",
      "\n",
      "Test set: Average loss: 0.2693, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 371 [0/1649 (0%)]\tLoss: 0.051121\n",
      "\n",
      "Test set: Average loss: 0.2640, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 372 [0/1649 (0%)]\tLoss: 0.112526\n",
      "\n",
      "Test set: Average loss: 0.2556, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 373 [0/1649 (0%)]\tLoss: 0.153349\n",
      "\n",
      "Test set: Average loss: 0.2332, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 374 [0/1649 (0%)]\tLoss: 0.035157\n",
      "\n",
      "Test set: Average loss: 0.2305, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 375 [0/1649 (0%)]\tLoss: 0.076684\n",
      "\n",
      "Test set: Average loss: 0.2291, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 376 [0/1649 (0%)]\tLoss: 0.073895\n",
      "\n",
      "Test set: Average loss: 0.2279, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 377 [0/1649 (0%)]\tLoss: 0.034918\n",
      "\n",
      "Test set: Average loss: 0.2301, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 378 [0/1649 (0%)]\tLoss: 0.263524\n",
      "\n",
      "Test set: Average loss: 0.2241, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 379 [0/1649 (0%)]\tLoss: 0.019444\n",
      "\n",
      "Test set: Average loss: 0.2272, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 380 [0/1649 (0%)]\tLoss: 0.029951\n",
      "\n",
      "Test set: Average loss: 0.2286, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 381 [0/1649 (0%)]\tLoss: 0.036773\n",
      "\n",
      "Test set: Average loss: 0.2335, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 382 [0/1649 (0%)]\tLoss: 0.065728\n",
      "\n",
      "Test set: Average loss: 0.2379, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 383 [0/1649 (0%)]\tLoss: 0.047819\n",
      "\n",
      "Test set: Average loss: 0.2456, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 384 [0/1649 (0%)]\tLoss: 0.038109\n",
      "\n",
      "Test set: Average loss: 0.2462, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 385 [0/1649 (0%)]\tLoss: 0.267302\n",
      "\n",
      "Test set: Average loss: 0.2194, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 386 [0/1649 (0%)]\tLoss: 0.025281\n",
      "\n",
      "Test set: Average loss: 0.2078, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 387 [0/1649 (0%)]\tLoss: 0.024778\n",
      "\n",
      "Test set: Average loss: 0.2029, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 388 [0/1649 (0%)]\tLoss: 0.030680\n",
      "\n",
      "Test set: Average loss: 0.2039, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 389 [0/1649 (0%)]\tLoss: 0.038016\n",
      "\n",
      "Test set: Average loss: 0.2049, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 390 [0/1649 (0%)]\tLoss: 0.033140\n",
      "\n",
      "Test set: Average loss: 0.2079, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 391 [0/1649 (0%)]\tLoss: 0.011770\n",
      "\n",
      "Test set: Average loss: 0.2136, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 392 [0/1649 (0%)]\tLoss: 0.080619\n",
      "\n",
      "Test set: Average loss: 0.2188, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 393 [0/1649 (0%)]\tLoss: 0.104590\n",
      "\n",
      "Test set: Average loss: 0.2184, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 394 [0/1649 (0%)]\tLoss: 0.022641\n",
      "\n",
      "Test set: Average loss: 0.2222, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 395 [0/1649 (0%)]\tLoss: 0.063230\n",
      "\n",
      "Test set: Average loss: 0.2252, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 396 [0/1649 (0%)]\tLoss: 0.045533\n",
      "\n",
      "Test set: Average loss: 0.2302, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 397 [0/1649 (0%)]\tLoss: 0.052832\n",
      "\n",
      "Test set: Average loss: 0.2294, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 398 [0/1649 (0%)]\tLoss: 0.074550\n",
      "\n",
      "Test set: Average loss: 0.2275, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 399 [0/1649 (0%)]\tLoss: 0.056788\n",
      "\n",
      "Test set: Average loss: 0.2214, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 400 [0/1649 (0%)]\tLoss: 0.054239\n",
      "\n",
      "Test set: Average loss: 0.2092, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 401 [0/1649 (0%)]\tLoss: 0.054866\n",
      "\n",
      "Test set: Average loss: 0.2010, Accuracy: 399/412 (97%)\n",
      "\n",
      "Train Epoch: 402 [0/1649 (0%)]\tLoss: 0.030873\n",
      "\n",
      "Test set: Average loss: 0.2072, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 403 [0/1649 (0%)]\tLoss: 0.007802\n",
      "\n",
      "Test set: Average loss: 0.2307, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 404 [0/1649 (0%)]\tLoss: 0.047993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2452, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 405 [0/1649 (0%)]\tLoss: 0.074731\n",
      "\n",
      "Test set: Average loss: 0.2354, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 406 [0/1649 (0%)]\tLoss: 0.051228\n",
      "\n",
      "Test set: Average loss: 0.2151, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 407 [0/1649 (0%)]\tLoss: 0.045795\n",
      "\n",
      "Test set: Average loss: 0.2043, Accuracy: 400/412 (97%)\n",
      "\n",
      "Train Epoch: 408 [0/1649 (0%)]\tLoss: 0.018226\n",
      "\n",
      "Test set: Average loss: 0.2062, Accuracy: 399/412 (97%)\n",
      "\n",
      "Train Epoch: 409 [0/1649 (0%)]\tLoss: 0.054791\n",
      "\n",
      "Test set: Average loss: 0.2112, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 410 [0/1649 (0%)]\tLoss: 0.142169\n",
      "\n",
      "Test set: Average loss: 0.2178, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 411 [0/1649 (0%)]\tLoss: 0.023209\n",
      "\n",
      "Test set: Average loss: 0.2249, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 412 [0/1649 (0%)]\tLoss: 0.005608\n",
      "\n",
      "Test set: Average loss: 0.2325, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 413 [0/1649 (0%)]\tLoss: 0.045244\n",
      "\n",
      "Test set: Average loss: 0.2317, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 414 [0/1649 (0%)]\tLoss: 0.022434\n",
      "\n",
      "Test set: Average loss: 0.2302, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 415 [0/1649 (0%)]\tLoss: 0.011857\n",
      "\n",
      "Test set: Average loss: 0.2293, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 416 [0/1649 (0%)]\tLoss: 0.023962\n",
      "\n",
      "Test set: Average loss: 0.2298, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 417 [0/1649 (0%)]\tLoss: 0.040578\n",
      "\n",
      "Test set: Average loss: 0.2293, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 418 [0/1649 (0%)]\tLoss: 0.025523\n",
      "\n",
      "Test set: Average loss: 0.2269, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 419 [0/1649 (0%)]\tLoss: 0.044566\n",
      "\n",
      "Test set: Average loss: 0.2255, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 420 [0/1649 (0%)]\tLoss: 0.008398\n",
      "\n",
      "Test set: Average loss: 0.2317, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 421 [0/1649 (0%)]\tLoss: 0.098058\n",
      "\n",
      "Test set: Average loss: 0.2337, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 422 [0/1649 (0%)]\tLoss: 0.071482\n",
      "\n",
      "Test set: Average loss: 0.2352, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 423 [0/1649 (0%)]\tLoss: 0.129353\n",
      "\n",
      "Test set: Average loss: 0.2358, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 424 [0/1649 (0%)]\tLoss: 0.011966\n",
      "\n",
      "Test set: Average loss: 0.2421, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 425 [0/1649 (0%)]\tLoss: 0.013592\n",
      "\n",
      "Test set: Average loss: 0.2512, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 426 [0/1649 (0%)]\tLoss: 0.047614\n",
      "\n",
      "Test set: Average loss: 0.2539, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 427 [0/1649 (0%)]\tLoss: 0.007796\n",
      "\n",
      "Test set: Average loss: 0.2563, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 428 [0/1649 (0%)]\tLoss: 0.064662\n",
      "\n",
      "Test set: Average loss: 0.2470, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 429 [0/1649 (0%)]\tLoss: 0.123427\n",
      "\n",
      "Test set: Average loss: 0.2242, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 430 [0/1649 (0%)]\tLoss: 0.029407\n",
      "\n",
      "Test set: Average loss: 0.2153, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 431 [0/1649 (0%)]\tLoss: 0.038204\n",
      "\n",
      "Test set: Average loss: 0.2164, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 432 [0/1649 (0%)]\tLoss: 0.074936\n",
      "\n",
      "Test set: Average loss: 0.2218, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 433 [0/1649 (0%)]\tLoss: 0.103153\n",
      "\n",
      "Test set: Average loss: 0.2387, Accuracy: 399/412 (97%)\n",
      "\n",
      "Train Epoch: 434 [0/1649 (0%)]\tLoss: 0.050002\n",
      "\n",
      "Test set: Average loss: 0.2549, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 435 [0/1649 (0%)]\tLoss: 0.015338\n",
      "\n",
      "Test set: Average loss: 0.2730, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 436 [0/1649 (0%)]\tLoss: 0.008384\n",
      "\n",
      "Test set: Average loss: 0.2916, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 437 [0/1649 (0%)]\tLoss: 0.031185\n",
      "\n",
      "Test set: Average loss: 0.3166, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 438 [0/1649 (0%)]\tLoss: 0.041503\n",
      "\n",
      "Test set: Average loss: 0.3337, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 439 [0/1649 (0%)]\tLoss: 0.148911\n",
      "\n",
      "Test set: Average loss: 0.3233, Accuracy: 385/412 (93%)\n",
      "\n",
      "Train Epoch: 440 [0/1649 (0%)]\tLoss: 0.030585\n",
      "\n",
      "Test set: Average loss: 0.3129, Accuracy: 387/412 (94%)\n",
      "\n",
      "Train Epoch: 441 [0/1649 (0%)]\tLoss: 0.043754\n",
      "\n",
      "Test set: Average loss: 0.3013, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 442 [0/1649 (0%)]\tLoss: 0.019992\n",
      "\n",
      "Test set: Average loss: 0.2954, Accuracy: 391/412 (95%)\n",
      "\n",
      "Train Epoch: 443 [0/1649 (0%)]\tLoss: 0.041534\n",
      "\n",
      "Test set: Average loss: 0.2858, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 444 [0/1649 (0%)]\tLoss: 0.169584\n",
      "\n",
      "Test set: Average loss: 0.2732, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 445 [0/1649 (0%)]\tLoss: 0.017540\n",
      "\n",
      "Test set: Average loss: 0.2665, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 446 [0/1649 (0%)]\tLoss: 0.047299\n",
      "\n",
      "Test set: Average loss: 0.2649, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 447 [0/1649 (0%)]\tLoss: 0.021055\n",
      "\n",
      "Test set: Average loss: 0.2641, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 448 [0/1649 (0%)]\tLoss: 0.099557\n",
      "\n",
      "Test set: Average loss: 0.2534, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 449 [0/1649 (0%)]\tLoss: 0.139386\n",
      "\n",
      "Test set: Average loss: 0.2453, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 450 [0/1649 (0%)]\tLoss: 0.019096\n",
      "\n",
      "Test set: Average loss: 0.2408, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 451 [0/1649 (0%)]\tLoss: 0.024140\n",
      "\n",
      "Test set: Average loss: 0.2353, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 452 [0/1649 (0%)]\tLoss: 0.068682\n",
      "\n",
      "Test set: Average loss: 0.2343, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 453 [0/1649 (0%)]\tLoss: 0.019475\n",
      "\n",
      "Test set: Average loss: 0.2353, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 454 [0/1649 (0%)]\tLoss: 0.047235\n",
      "\n",
      "Test set: Average loss: 0.2377, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 455 [0/1649 (0%)]\tLoss: 0.012982\n",
      "\n",
      "Test set: Average loss: 0.2412, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 456 [0/1649 (0%)]\tLoss: 0.025066\n",
      "\n",
      "Test set: Average loss: 0.2424, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 457 [0/1649 (0%)]\tLoss: 0.008756\n",
      "\n",
      "Test set: Average loss: 0.2440, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 458 [0/1649 (0%)]\tLoss: 0.022490\n",
      "\n",
      "Test set: Average loss: 0.2449, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 459 [0/1649 (0%)]\tLoss: 0.032333\n",
      "\n",
      "Test set: Average loss: 0.2410, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 460 [0/1649 (0%)]\tLoss: 0.018970\n",
      "\n",
      "Test set: Average loss: 0.2383, Accuracy: 394/412 (96%)\n",
      "\n",
      "Train Epoch: 461 [0/1649 (0%)]\tLoss: 0.010839\n",
      "\n",
      "Test set: Average loss: 0.2373, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 462 [0/1649 (0%)]\tLoss: 0.038283\n",
      "\n",
      "Test set: Average loss: 0.2368, Accuracy: 399/412 (97%)\n",
      "\n",
      "Train Epoch: 463 [0/1649 (0%)]\tLoss: 0.097347\n",
      "\n",
      "Test set: Average loss: 0.2400, Accuracy: 399/412 (97%)\n",
      "\n",
      "Train Epoch: 464 [0/1649 (0%)]\tLoss: 0.021418\n",
      "\n",
      "Test set: Average loss: 0.2395, Accuracy: 399/412 (97%)\n",
      "\n",
      "Train Epoch: 465 [0/1649 (0%)]\tLoss: 0.105034\n",
      "\n",
      "Test set: Average loss: 0.2383, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 466 [0/1649 (0%)]\tLoss: 0.020479\n",
      "\n",
      "Test set: Average loss: 0.2379, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 467 [0/1649 (0%)]\tLoss: 0.005615\n",
      "\n",
      "Test set: Average loss: 0.2416, Accuracy: 392/412 (95%)\n",
      "\n",
      "Train Epoch: 468 [0/1649 (0%)]\tLoss: 0.028987\n",
      "\n",
      "Test set: Average loss: 0.2469, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 469 [0/1649 (0%)]\tLoss: 0.012286\n",
      "\n",
      "Test set: Average loss: 0.2518, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 470 [0/1649 (0%)]\tLoss: 0.011575\n",
      "\n",
      "Test set: Average loss: 0.2545, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 471 [0/1649 (0%)]\tLoss: 0.022226\n",
      "\n",
      "Test set: Average loss: 0.2567, Accuracy: 390/412 (95%)\n",
      "\n",
      "Train Epoch: 472 [0/1649 (0%)]\tLoss: 0.031677\n",
      "\n",
      "Test set: Average loss: 0.2569, Accuracy: 389/412 (94%)\n",
      "\n",
      "Train Epoch: 473 [0/1649 (0%)]\tLoss: 0.024552\n",
      "\n",
      "Test set: Average loss: 0.2597, Accuracy: 388/412 (94%)\n",
      "\n",
      "Train Epoch: 474 [0/1649 (0%)]\tLoss: 0.057283\n",
      "\n",
      "Test set: Average loss: 0.2541, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 475 [0/1649 (0%)]\tLoss: 0.033329\n",
      "\n",
      "Test set: Average loss: 0.2516, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 476 [0/1649 (0%)]\tLoss: 0.030706\n",
      "\n",
      "Test set: Average loss: 0.2496, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 477 [0/1649 (0%)]\tLoss: 0.033012\n",
      "\n",
      "Test set: Average loss: 0.2452, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 478 [0/1649 (0%)]\tLoss: 0.029140\n",
      "\n",
      "Test set: Average loss: 0.2417, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 479 [0/1649 (0%)]\tLoss: 0.003541\n",
      "\n",
      "Test set: Average loss: 0.2404, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 480 [0/1649 (0%)]\tLoss: 0.014114\n",
      "\n",
      "Test set: Average loss: 0.2387, Accuracy: 399/412 (97%)\n",
      "\n",
      "Train Epoch: 481 [0/1649 (0%)]\tLoss: 0.066394\n",
      "\n",
      "Test set: Average loss: 0.2335, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 482 [0/1649 (0%)]\tLoss: 0.020373\n",
      "\n",
      "Test set: Average loss: 0.2310, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 483 [0/1649 (0%)]\tLoss: 0.073525\n",
      "\n",
      "Test set: Average loss: 0.2337, Accuracy: 399/412 (97%)\n",
      "\n",
      "Train Epoch: 484 [0/1649 (0%)]\tLoss: 0.047785\n",
      "\n",
      "Test set: Average loss: 0.2297, Accuracy: 400/412 (97%)\n",
      "\n",
      "Train Epoch: 485 [0/1649 (0%)]\tLoss: 0.005434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2293, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 486 [0/1649 (0%)]\tLoss: 0.038098\n",
      "\n",
      "Test set: Average loss: 0.2325, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 487 [0/1649 (0%)]\tLoss: 0.025201\n",
      "\n",
      "Test set: Average loss: 0.2363, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 488 [0/1649 (0%)]\tLoss: 0.010339\n",
      "\n",
      "Test set: Average loss: 0.2413, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 489 [0/1649 (0%)]\tLoss: 0.087796\n",
      "\n",
      "Test set: Average loss: 0.2483, Accuracy: 398/412 (97%)\n",
      "\n",
      "Train Epoch: 490 [0/1649 (0%)]\tLoss: 0.010684\n",
      "\n",
      "Test set: Average loss: 0.2568, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 491 [0/1649 (0%)]\tLoss: 0.056561\n",
      "\n",
      "Test set: Average loss: 0.2619, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 492 [0/1649 (0%)]\tLoss: 0.014601\n",
      "\n",
      "Test set: Average loss: 0.2690, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 493 [0/1649 (0%)]\tLoss: 0.001460\n",
      "\n",
      "Test set: Average loss: 0.2774, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 494 [0/1649 (0%)]\tLoss: 0.021019\n",
      "\n",
      "Test set: Average loss: 0.2833, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 495 [0/1649 (0%)]\tLoss: 0.068278\n",
      "\n",
      "Test set: Average loss: 0.2803, Accuracy: 397/412 (96%)\n",
      "\n",
      "Train Epoch: 496 [0/1649 (0%)]\tLoss: 0.054693\n",
      "\n",
      "Test set: Average loss: 0.2832, Accuracy: 396/412 (96%)\n",
      "\n",
      "Train Epoch: 497 [0/1649 (0%)]\tLoss: 0.013999\n",
      "\n",
      "Test set: Average loss: 0.2896, Accuracy: 395/412 (96%)\n",
      "\n",
      "Train Epoch: 498 [0/1649 (0%)]\tLoss: 0.061406\n",
      "\n",
      "Test set: Average loss: 0.3006, Accuracy: 393/412 (95%)\n",
      "\n",
      "Train Epoch: 499 [0/1649 (0%)]\tLoss: 0.074704\n",
      "\n",
      "Test set: Average loss: 0.3016, Accuracy: 392/412 (95%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_arr = range(500)\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "#Entrainement\n",
    "for epoch in epoch_arr:\n",
    "    train_loss.append( train(model, dataloader_train, optimizer, epoch, F.nll_loss))\n",
    "    test_loss.append( test(model, dataloader_test) )\n",
    "#Affichage des courbes de loss\n",
    "plt.plot(epoch_arr,train_loss,'b')\n",
    "plt.plot(epoch_arr,test_loss,'g')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
